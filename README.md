# Data modeling with postgres

## Introduction

In this project, we set up a simple ETL pipeline to centralize song's data on a postgres database and make analysis easy for the *Sparkly* startup.


## Data

### Song Dataset
The first dataset is a subset of real data from the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The files are partitioned by the first three letters of each song's track ID.

### Log Dataset
The second dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations.

## Metodoloty

Using the song and log datasets, we created a star schema optimized for queries on song play analysis. This includes the following tables.

- Fact Table
   - songplays - records in log data associated with song plays i.e. records with page NextSong

- Dimension Tables
   - users - users in the app
   - songs - songs in music database
   - artists - artists in music database
   - time - timestamps of records in songplays broken down into specific units

## Files

There are three main python scripts on this project:

1. `sql_queries.py` has all the CRUD queries needed for each table.
2. `create_tables.py` creates the database, establish the connection and creates/drops all the tables required using sql_queries module.
3. `etl.py` build the pipeline that extracts the data from JSON files, does some transformation and then insert all the data into the corresponding tables.

## HOW-TO

Once you have your database up and running all you need to do is to run the followinf commands on the project directory:

```
python create_tables.py    
python etl.py
```

